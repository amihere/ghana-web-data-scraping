{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import cloudscraper\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pymysql\n",
    "from ulid import ULID\n",
    "from datetime import datetime\n",
    "from sqlalchemy import types, create_engine\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load env\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "load_dotenv(os.path.join(BASE_DIR, '.env'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve section date from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYSQL_DB_USER = os.getenv(\"MYSQL_DB_USER\")\n",
    "MYSQL_DB_PASSWORD = os.getenv(\"MYSQL_DB_PASSWORD\")\n",
    "MYSQL_DB_HOST = os.getenv(\"MYSQL_DB_HOST\")\n",
    "MYSQL_DB_PORT = int(os.getenv(\"MYSQL_DB_PORT\"))\n",
    "MYSQL_DB_NAME = os.getenv(\"MYSQL_DB_NAME\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(host=MYSQL_DB_HOST,\n",
    "                       port=MYSQL_DB_PORT,\n",
    "                       user=MYSQL_DB_USER,\n",
    "                       passwd=MYSQL_DB_PASSWORD,\n",
    "                       db=MYSQL_DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##finding last time table was updated\n",
    "section_data = pd.read_sql_query(\"Select id,url from sections where section = 'Abroad, Ghanaians'\", con=conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting story URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = ['sectionID','url','storyExtracted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story_urls(row):\n",
    "    # try:\n",
    "    df = pd.DataFrame()\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    page = scraper.get(row[\"url\"])\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    page_stories = soup.find_all(\"div\", class_ = \"afcon-news list\")\n",
    "    for ul in page_stories:\n",
    "        for li in ul.findAll('li'):\n",
    "            data = li.find(\"a\")\n",
    "            url = row['url']+data.get('href')\n",
    "            story_extracted = \"PENDING\"\n",
    "            temp_row = pd.DataFrame(data = [[row[\"id\"], url,story_extracted]], columns = df_columns)\n",
    "            df = pd.concat([df,temp_row], ignore_index=True)  \n",
    "        time.sleep(20)\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df  = section_data.apply(lambda row : get_story_urls(row),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_urls_df = pd.DataFrame(columns = df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in url_df:\n",
    "    article_urls_df = pd.concat([article_urls_df,row])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ulid to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to generate ulid\n",
    "def generate_ulid(row):\n",
    "    row[\"id\"] = int(ULID())\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_urls_df = article_urls_df.apply(lambda row: generate_ulid(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_urls_df = article_urls_df[[\"id\",\"url\",\"sectionID\",\"storyExtracted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_urls_df[\"dateGenerated\"] = datetime.now()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_engine = create_engine(f\"mysql+pymysql://{MYSQL_DB_USER}:{MYSQL_DB_PASSWORD}@{MYSQL_DB_HOST}:{MYSQL_DB_PORT}/{MYSQL_DB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create df schema\n",
    "df_schema = dict(zip(article_urls_df.columns.tolist(),(types.VARCHAR(length=100), \n",
    "                                                    types.VARCHAR(length=1000),\n",
    "                                                    types.VARCHAR(length=100),\n",
    "                                                    types.VARCHAR(length=100),\n",
    "                                                    types.TIMESTAMP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_urls_df.to_sql(\"story_urls\", con=mysql_engine, if_exists=\"append\",dtype=df_schema,index=False) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Stories from urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# story_df_columns = ['Section','Header','Date','Title','Information']\n",
    "# stories_data = pd.DataFrame(columns = story_df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_abroad_gh_articles(section_title,url):\n",
    "#     df = pd.DataFrame()\n",
    "#     scraper = cloudscraper.create_scraper()\n",
    "#     page = scraper.get(url)\n",
    "#     soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#     page_stories = soup.find(id = \"medsection1\")\n",
    "#     try:\n",
    "#         div_class = page_stories.find(\"div\", class_ = \"article-left-col\")       \n",
    "#     except AttributeError:\n",
    "#         print('cannot find the target div: article-left-col')\n",
    "#         file_object = open(\"C:\\\\Users\\\\annieboadu\\\\Documents\\\\ghana-web-data-main\\\\error_urls\\\\article urls.txt\", 'a')\n",
    "#         file_object.write(f'\\n\\n{section_title},{url}')\n",
    "#         file_object.close()\n",
    "#         return None\n",
    "#     else:\n",
    "#         header = div_class.find('p', class_ = \"floatLeft\").text\n",
    "#         date = div_class.find('a', id = \"date\").text\n",
    "#         title = div_class.find('h1').text\n",
    "#         info = div_class.find('p',id = \"article-123\").text\n",
    "#         temp_row = pd.DataFrame(data = [[section_title, header, date, title, info]], columns = story_df_columns)\n",
    "#         df = pd.concat([df,temp_row],ignore_index =True)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abroad_gh_final_df = article_urls_df.apply(lambda row:get_abroad_gh_articles(row['Section Title'],row['Url']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abroad_stories_urls_df = pd.DataFrame(columns = story_df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in abroad_gh_final_df:\n",
    "#     abroad_stories_urls_df = pd.concat([abroad_stories_urls_df,row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abroad_stories_urls_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del abroad_stories_urls_df[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3983a69a71ba65f379ca11b7271da0d7fd4a5f5f998a2bdf28810253fd0e4405"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
